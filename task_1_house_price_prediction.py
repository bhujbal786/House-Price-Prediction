# -*- coding: utf-8 -*-
"""Task 1 : House Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qOM7Y23AdKbuRtfTeRYSqyYzCILr05u6
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
from sklearn.linear_model import LinearRegression
# %matplotlib inline
from sklearn.linear_model import Ridge

from google.colab import files

# Prompt the user to select the uploaded file
uploaded = files.upload()

# Get the file name
filename = list(uploaded.keys())[0]

# Load the CSV file into a Pandas DataFrame
import pandas as pd
df = pd.read_csv('kc_house_data.csv')

# Display the DataFrame
df.head()

df = pd.read_csv('kc_house_data.csv')

df.head()

df.shape

df.dtypes

df.describe().transpose()

df_desc = df.describe()
q1 = df_desc.loc['25%']
q3 = df_desc.loc['75%']
iqr = q3 - q1
df_desc.loc['lower_bound'] = q1 - 1.5 * iqr
df_desc.loc['upper_bound'] = q3 + 1.5 * iqr

df_desc.transpose()

df.isnull().sum()

mean=df['bedrooms'].mean()
df['bedrooms'].replace(np.nan,mean, inplace=True)

mean=df['bathrooms'].mean()
df['bathrooms'].replace(np.nan,mean, inplace=True)

print("number of NaN values for the column bedrooms :", df['bedrooms'].isnull().sum())
print("number of NaN values for the column bathrooms :", df['bathrooms'].isnull().sum())

# Select columns for box plot
cols = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']

# Create subplots
fig, axes = plt.subplots(1, len(cols), figsize=(20, 5))

# Generate boxplots for each column
for i, col in enumerate(cols):
    sns.boxplot(y=col, data=df, ax=axes[i])
    axes[i].set_title(col, fontsize=14)
    axes[i].set_ylabel('')
    
# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# Remove outliers for price column
price_upper_limit = 5000000
df = df[df['price'] <= price_upper_limit]

# Remove outliers for bedrooms column
bedrooms_upper_limit = 8
df = df[df['bedrooms'] <= bedrooms_upper_limit]

# Remove outliers for bathrooms column
bathrooms_upper_limit = 6
df = df[(df['bathrooms'] < bathrooms_upper_limit) & (df['bathrooms'] >= 1)]

# Remove outliers for sqft_living column
sqft_living_upper_limit = 8500
df = df[df['sqft_living'] <= sqft_living_upper_limit]

# Remove outliers for sqft_lot column
sqft_lot_upper_limit = 60000
df = df[df['sqft_lot'] <= sqft_lot_upper_limit]

# Remove outliers for sqft_above column
sqft_above_upper_limit = 7000
df = df[df['sqft_above'] <= sqft_above_upper_limit]

# Remove outliers for sqft_basement column
sqft_basement_upper_limit = 3000
df = df[df['sqft_basement'] <= sqft_basement_upper_limit]

df.describe().transpose()

# Define the figure and subplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Create count plots for each variable and add them to the subplots
sns.countplot(x='bedrooms', data=df, ax=axes[0,0])
sns.countplot(x='bathrooms', data=df, ax=axes[0,1])
sns.countplot(x='floors', data=df, ax=axes[0,2])
sns.countplot(x='waterfront', data=df, ax=axes[1,0])
sns.countplot(x='view', data=df, ax=axes[1,1])

# Set the title for the entire plot
fig.suptitle("Count Plots for Bedrooms, Bathrooms, Floors, Waterfront, and View")

# Show the plot
plt.show()

# Identify skewed features
skewed_feats = ['price', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']

# Log-transform skewed features
for feat in skewed_feats:
    df[feat] = np.log1p(df[feat])

# Visualize the distribution of transformed features
fig, axs = plt.subplots(2, 3, figsize=(15,10))
axs = axs.ravel()
for i, feat in enumerate(skewed_feats):
    axs[i].hist(df[feat], bins=50, color='blue', alpha=0.5)
    axs[i].set_title(feat + ' Distribution', fontsize=12)
    axs[i].set_xlabel('Value', fontsize=10)
    axs[i].set_ylabel('Frequency', fontsize=10)
plt.tight_layout()
plt.show()

# Create a correlation matrix for numerical variables
corr_matrix = df.corr()

# Visualize the correlation matrix using a heatmap
fig, ax = plt.subplots(figsize=(14, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)

df = df.drop(["zipcode", "yr_built", "condition"], axis = 1)
df.head()

Floor_Value_counts = df["floors"].value_counts().to_frame()
Floor_Value_counts

sns.boxplot(x= "waterfront", y = "price", data = df)

sns.regplot(x="sqft_above", y ="price", data = df)

X = df[['long']]
Y = df['price']
lm = LinearRegression()
lm.fit(X,Y)
lm.score(X, Y)

X = df[['sqft_living']]
Y = df['price']
lm = LinearRegression()
lm.fit(X,Y)
lm.score(X, Y)

features =df[["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]]
mlm = LinearRegression()
mlm.fit(features, Y)

mlm.score(features, Y)

Input=[('scale',StandardScaler()),('polynomial', PolynomialFeatures(include_bias=False)),('model',LinearRegression())]

pipe = Pipeline(Input)

pipe.fit(features, Y)
yhat = pipe.predict(features)
yhat

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
print("done")

features =["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]    
X = df[features]
Y = df['price']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1)


print("number of test samples:", x_test.shape[0])
print("number of training samples:",x_train.shape[0])

RidgeModel = Ridge(alpha= 0.1)
RidgeModel.fit(x_train, y_train)
RidgeModel.score(x_test,y_test)

from sklearn.metrics import r2_score
poly = PolynomialFeatures(degree=2)
x_train_poly = poly.fit_transform(x_train)
x_test_poly = poly.fit_transform(x_test)

ridge = Ridge(alpha=0.1)
ridge.fit(x_train_poly, y_train)

# predict on test data and calculate R^2 score
y_test_pred = ridge.predict(x_test_poly)
r2 = r2_score(y_test, y_test_pred)

print("R^2 score:", r2)

from sklearn.svm import SVR

svm_reg = SVR()
svm_reg.fit(x_train, y_train)
svm_reg_score = svm_reg.score(x_test, y_test)
print(f"SVM R^2: {svm_reg_score}")

linear_reg = LinearRegression()
linear_reg.fit(x_train, y_train)
linear_reg_score = linear_reg.score(x_test, y_test)
print(f"Linear Regression R^2: {linear_reg_score}")

ridge_reg = Ridge(alpha=0.1)
ridge_reg.fit(x_train, y_train)
ridge_reg_score = ridge_reg.score(x_test, y_test)
print(f"Ridge Regression R^2: {ridge_reg_score}")

from sklearn.linear_model import Lasso

lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(x_train, y_train)
lasso_reg_score = lasso_reg.score(x_test, y_test)
print(f"Lasso Regression R^2: {lasso_reg_score}")

from sklearn.tree import DecisionTreeRegressor

decision_tree = DecisionTreeRegressor()
decision_tree.fit(x_train, y_train)
decision_tree_score = decision_tree.score(x_test, y_test)
print(f"Decision Tree R^2: {decision_tree_score}")

from sklearn.ensemble import RandomForestRegressor

random_forest = RandomForestRegressor()
random_forest.fit(x_train, y_train)
random_forest_score = random_forest.score(x_test, y_test)
print(f"Random Forest R^2: {random_forest_score}")

from sklearn.model_selection import GridSearchCV

# Define the hyperparameter grid for Random Forest
param_grid = {
    'n_estimators': [10, 50, 100, 200],
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False],
}

# Create the Random Forest model
random_forest = RandomForestRegressor()

# Initialize GridSearchCV with the model and the hyperparameter grid
grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid,
                           cv=5, verbose=1, n_jobs=-1)

# Fit the grid search to the training data
grid_search.fit(x_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best hyperparameters: {best_params}")

# Evaluate the best model on the test data
best_model = grid_search.best_estimator_
best_model_score = best_model.score(x_test, y_test)
print(f"Random Forest with optimized hyperparameters R^2: {best_model_score}")